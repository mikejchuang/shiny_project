df1 <- df %>% group_by(referred_ID, story) %>% summarise(sum(n()))
df1
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% summarise(sum(n()))
df1
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% summarise(count=n()))
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% summarise(count=n())
df1
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% mutate(count=n())
df1
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(story) %>% mutate(count=n())
df1
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% mutate(count=n())
df1
#find referred IDs of comments
df <- shares %>% mutate(story=ifelse(type=='Comment',1,0))
df
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% mutate(count=n())
df1
#find referred IDs of comments
df <- shares %>% mutate(story=ifelse(type=='Comment',1,0))
df
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% mutate(count=n())
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% mutate(sum(story))
df1
#find referred IDs of comments
df <- shares %>% mutate(story=ifelse(type=='Comment',1,0))
df
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% mutate(sum(story))
df1
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% mutate(sum(story))
df1
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% mutate(story_num=sum(story))
df1
df2 <- df1 %>% group_by(story_sum)
df2 <- df1 %>% group_by(story_num)
df2
df2 <- df1 %>% group_by(story_num)%>% summarise(count = n())
df2
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% summarise(story_num=sum(story))
df1
df2 <- df1 %>% group_by(story_num)%>% summarise(count = n())
df2
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% summarise(story_num=sum(story))
df1
df1
df2 <- df1 %>% group_by(story_num)%>% summarise(count = n())
df2
Facebook %>% mutate(., Commented = ifelse(Type == “Comment”, 1, 0)) %>% group_by(., ReferredID) %>%
summarise(., Numb_of_Comments = sum(Commented)) %>% group_by(., Numb_of_Comments) %>%
summarise(., Numb_of_Stories = n())
#find referred IDs of comments
df <- shares %>% mutate(story=ifelse(type=='Comment',1,0))
df
#Find stories where df$referred_ID is in shares$action_ID
df1 <- df %>% group_by(referred_ID) %>% summarise(story_num=sum(story))
df1
df2 <- df1 %>% group_by(story_num)%>% summarise(count = n())
df2
shares = as.data.frame(cbind(
FacebookID = c(17998,29830,30980,23089),
action_ID = c(1001,981,734,985),
type = c('Post','Comment','Photo','Share'),
referred_ID = c(345,1001,234,1001)))
%>% summarise(Number_of_stories = n())
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n()-1)
library(dplyr)
shares = as.data.frame(cbind(
FacebookID = c(17998,29830,30980,23089),
action_ID = c(1001,981,734,985),
type = c('Post','Comment','Photo','Share'),
referred_ID = c(345,1001,234,1001)))
library(dplyr)
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n()-1)
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n())
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n()-1)
%>% group_by(comments)
%>% summarise(Number_of_stories = n())
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n()-1)
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n()-1) %>% group_by(comments)
%>% summarise(Number_of_stories = n())
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n()-1) %>% group_by(comments)
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n()-1) %>% group_by(comments)
shares %>% group_by(.,referred_ID) %>%
summarise(comments = n()-1) %>% group_by(comments)%>% summarise(Number_of_stories = n())
# Bernoulli random walk
# rbinom with size = 1 is nothing but Bernoulli distribution
epsilon_ber <- 2*rbinom(100,1,p=0.5)-1     # converting the values 0, 1 to -1, 1
##taking 100 samples, like flipping a random coin with p=0.5, multiplying by -1 makes it go from -1,1 instead of 0,1
x_ber <- cumsum(epsilon_ber)
plot(x_ber, type='l')
epsilon_norm <- rnorm(100, 0, sd=0.1)
x_norm <- cumsum(epsilon_norm)
plot(x_norm, type='l')
N<-10000
plot(x_norm, type='l')
N<-10000
epsilon1_ber <-  2 * rbinom(N,1,p=0.5) - 1
epsilon2_ber <-  2 * rbinom(N,1,p=0.5) - 1
x_ber <-  cumsum(epsilon1_ber)
y_ber <-  cumsum(epsilon2_ber)
plot(x_ber, y_ber, type='l')
N<-10000
epsilon1_norm <-  rnorm(N,0,0.1)
epsilon2_norm <-  rnorm(N,0,0.1)
x_norm <-  cumsum(epsilon1_norm)
y_norm <-  cumsum(epsilon2_norm)
plot(x_norm, y_norm, type='l')
plot(x_norm, y_norm, type='l')
N<-10000
epsilon1_norm <-  rnorm(N,0,0.1)
epsilon2_norm <-  rnorm(N,0,0.1)
x_norm <-  cumsum(epsilon1_norm)
y_norm <-  cumsum(epsilon2_norm)
plot(x_norm, y_norm, type='l')
# SnP500 index price series
library(quantmod)
install.packages("quantmod")
# SnP500 index price series
library(quantmod)
sp500<-new.env()
startDate = as.Date('1960-01-04')
endDate   = as.Date('2017-11-26')
getSymbols('^GSPC',env=sp500, src='yahoo', from=startDate,
to=endDate,auto.assign=T)
# No dividend for a stock index, but useful for the individual stocks
getDividends('^GSPC', env=sp500, src='yahoo', from=startDate,
to=endDate,auto.assign=T)
# No split for a stock index, but useful for the individual stocks
getSplits('^GSPC', env=sp500, src='yahoo',from=startDate,to=endDate,auto.assign=T)
head(sp500$GSPC)
sp500<-new.env()
startDate = as.Date('1960-01-04')
endDate   = as.Date('2017-11-26')
getSymbols('^GSPC',env=sp500, src='yahoo', from=startDate,
to=endDate,auto.assign=T)
# No dividend for a stock index, but useful for the individual stocks
getDividends('^GSPC', env=sp500, src='yahoo', from=startDate,
to=endDate,auto.assign=T)
# No split for a stock index, but useful for the individual stocks
getSplits('^GSPC', env=sp500, src='yahoo',from=startDate,to=endDate,auto.assign=T)
head(sp500$GSPC)
tail(sp500$GSPC)
class(sp500$GSPC)
sp_prc<-sp500$GSPC[,6]
class(sp_prc)    # xts, zoo      'zoo' stands for Z's ordered observations
sp_prc<-sp500$GSPC[,6]
class(sp_prc)    # xts, zoo      'zoo' stands for Z's ordered observations
#1960-01-04         59.91
#2017-11-24       2602.42
# total return    (2446.30-59.91)/59.91 ~ 39.83292
# daily return
plot(sp_prc)
# xts can handle irregular period time series as well as regular time series
sp_prc
#1960-01-04         59.91
#2017-11-24       2602.42
# total return    (2446.30-59.91)/59.91 ~ 39.83292
# daily return
plot(sp_prc)
ret_sp500_daily<-(sp_prc-lag(sp_prc))/lag(sp_prc)
plot(ret_sp500_daily)
plot(ret_sp500_daily)
mean(ret_sp500_daily,na.rm=T) * 252     # annualized return
spy<-new.env()
getSymbols('SPY',env=spy, src='yahoo', from=as.Date('1993-01-29'),
to=as.Date('2017-11-26'))
getDividends('SPY',env=spy, src='yahoo', from=as.Date('1993-01-29'),
to=as.Date('2017-11-26'),auto.assign=T)
getSplits('SPY',env=spy, src='yahoo', from=as.Date('1993-01-29'),
to=as.Date('2017-11-26'),auto.assign=T)
head(spy$SPY)
spy_prc <- spy$SPY[,6]
ret_spy_daily <- diff(spy_prc)/lag(spy_prc)
ret_spy_div   <- spy$SPY.div/spy_prc
mean(ret_spy_daily, na.rm=T) * 252    # ~ 10.74%/year
mean(ret_spy_div,na.rm=T) * 4 # one dividend per quarter 2.297%/year
# rollmean
sp_avg20 = rollmean(sp_prc, 20, align='center')
plot(sp_avg20)
sp_avg100 = rollmean(sp_prc, 100, align='center')
plot(sp_avg100)
ret_sp500_daily<-ret_sp500_daily[2:length(ret_sp500_daily)]  # removing the first nan element
ret_sp500_avg20 = rollmean(ret_sp500_daily, 20, align='center')
plot(ret_sp500_avg20)
ret_sp500_avg100 = rollmean(ret_sp500_daily, 100, align='center')
plot(ret_sp500_avg100)       # The 100 trading days moving average is smoother than the 20 days version
# The daily return of SP500 has a big fat tail
qqnorm(ret_sp500_daily)
qqline(ret_sp500_daily)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.2
X[1]  <- 0.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
head(spy$SPY)
spy_prc <- spy$SPY[,6]
ret_spy_daily <- diff(spy_prc)/lag(spy_prc)
ret_spy_div   <- spy$SPY.div/spy_prc
mean(ret_spy_daily, na.rm=T) * 252    # ~ 10.74%/year
mean(ret_spy_div,na.rm=T) * 4 # one dividend per quarter 2.297%/year
# rollmean
sp_avg20 = rollmean(sp_prc, 20, align='center')
plot(sp_avg20)
ret_sp500_daily<-ret_sp500_daily[2:length(ret_sp500_daily)]  # removing the first nan element
ret_sp500_avg20 = rollmean(ret_sp500_daily, 20, align='center')
plot(ret_sp500_avg20)
plot(ret_sp500_avg20)
ret_sp500_avg100 = rollmean(ret_sp500_daily, 100, align='center')
plot(ret_sp500_avg100)       # The 100 trading days moving average is smoother than the 20 days version
sp_avg100 = rollmean(sp_prc, 50, align='center')
plot(sp_avg100)
ret_sp500_avg100 = rollmean(ret_sp500_daily, 50, align='center')
plot(ret_sp500_avg100)       # The 100 trading days moving average is smoother than the 20 days version
ret_sp500_avg100 = rollmean(ret_sp500_daily, 100, align='center')
plot(ret_sp500_avg100)       # The 100 trading days moving average is smoother than the 20 days version
# The daily return of SP500 has a big fat tail
qqnorm(ret_sp500_daily)
qqline(ret_sp500_daily)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.2
X[1]  <- 0.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
plot(X,type='l')
library(tseries)
install.packages("tseries")
library(tseries)
# test for stationary
# adf.test
adf.test(sp_prc)
adf.test(ret_sp500_daily)
# test for stationary
# adf.test
adf.test(sp_prc)
adf.test(ret_sp500_daily)
#
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.2
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
set.seed(0)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.95
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
set.seed(0)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.95
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
set.seed(0)
N <- 1000
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.95
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
set.seed(0)
N <- 100
sigma <- 0.01
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.95
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.5
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
acf(X, lag.max=10)  # lag.max = 10 to plot only the 10 terms
N <- 1000
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.5
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
acf(X, lag.max=10)  # The noise in the auto-correlation function reduces as N goes large
# acf on the sp500 daily return
acf(ret_sp500_daily, lag.max=100)   # The stock index is generally efficient, there is no long term pattern
acf(X, lag.max=10)  # The noise in the auto-correlation function reduces as N goes large
# acf on the sp500 daily return
acf(ret_sp500_daily, lag.max=100)   # The stock index is generally efficient, there is no long term pattern
result_theta<-rollapply(as.vector(ret_sp500_daily),width=100,FUN=acf,lag.max=1,
type='correlation',plot=FALSE, align='right')
# result_theta is a matrix
M<-length(result_theta[,1])
thetas <- numeric(M)
for (i in 1:M) {
thetas[i] <- result_theta[i,]$acf[2]
}
sp500_vola_daily <-rollapply(ret_sp500_daily, width=100, FUN=sd)
plot(sp500_vola_daily)
thetas <- as.xts(thetas, order.by=index(sp500_vola_daily[100:(M+99)]))
plot(thetas)
# Some EDA on the relationship between volatility and thetas
return_vola_100 <- sp500_vola_daily[100:(M+99)]
return_vola     <- as.vector(return_vola_100)
return_autocorr <- as.vector(thetas)
return_daily_100    <- as.vector(rollmean(ret_sp500_daily, 100, align='right'))
plot(return_daily_100, return_vola)
plot(return_vola, return_autocorr)
plot(return_daily_100, type='l')
plot(return_vola_100, type='l')
result_theta<-rollapply(as.vector(ret_sp500_daily),width=100,FUN=acf,lag.max=1,
type='correlation',plot=FALSE, align='right')
# result_theta is a matrix
M<-length(result_theta[,1])
thetas <- numeric(M)
for (i in 1:M) {
thetas[i] <- result_theta[i,]$acf[2]
}
sp500_vola_daily <-rollapply(ret_sp500_daily, width=100, FUN=sd)
plot(sp500_vola_daily)
adf.test(return_vola_100)  #likely to be stationary
plot(return_daily_100, type='l')
plot(return_vola_100, type='l')
adf.test(return_vola_100)  #likely to be stationary
#install.packages('forecast')
library(forecast)
install.packages("forecast")
# Some EDA on the relationship between volatility and thetas
return_vola_100 <- sp500_vola_daily[100:(M+99)]
return_vola     <- as.vector(return_vola_100)
return_autocorr <- as.vector(thetas)
return_daily_100    <- as.vector(rollmean(ret_sp500_daily, 100, align='right'))
plot(return_daily_100, return_vola)
plot(return_vola, return_autocorr)
plot(return_daily_100, type='l')
plot(return_vola_100, type='l')
adf.test(return_vola_100)  #likely to be stationary
auto.arima(return_vola_100)  # default kpss test rejects the null hypothesis of stationary time series
#install.packages('forecast')
library(forecast)
auto.arima(return_vola_100)  # default kpss test rejects the null hypothesis of stationary time series
#install.packages('forecast')
library(forecast)
auto.arima(return_vola_100)  # default kpss test rejects the null hypothesis of stationary time series
auto.arima(return_vola_100, test='adf')
sample <- seq(100, 14399, 100)
ret_vola_sampled <- return_vola_100[sample]
auto.arima(ret_vola_sampled, test='adf', trace=T)  #ARIMA(1,0,0) with non-zero mean
# compared with
auto.arima(ret_vola_sampled, trace=T)  # ARIMA(1,1,1) by using the kpss test intead of adf test
#
auto.arima(ret_vola_sampled^2, test='adf', trace=T)   # Don't forget Box-Cox!
acf(ret_vola_sampled^2)
sample <- seq(100, 14399, 100)
ret_vola_sampled <- return_vola_100[sample]
auto.arima(ret_vola_sampled, test='adf', trace=T)  #ARIMA(1,0,0) with non-zero mean
auto.arima(ret_vola_sampled, test='adf', trace=T)  #ARIMA(1,0,0) with non-zero mean
sample <- seq(100, 14399, 100)
ret_vola_sampled <- return_vola_100[sample]
auto.arima(ret_vola_sampled, test='adf', trace=T)  #ARIMA(1,0,0) with non-zero mean
# compared with
auto.arima(ret_vola_sampled, trace=T)  # ARIMA(1,1,1) by using the kpss test intead of adf test
N <- 1000
epsilon <- rnorm(N, 0, 0.1)
X <- numeric(N)
for (i in 3:N) {
X[i] <- 0.5 * X[i-2] + epsilon[i]
}
for (i in 3:N) {
X[i] <- 0.5 * X[i-2] + epsilon[i]
}
# compare acf vs pacf
acf(X, lag.max=20)
pacf(X, lag.max=20)
# compare acf vs pacf
acf(X, lag.max=20)
pacf(X, lag.max=20)
acf(X, lag.max=20)  # Notice that acf drops to nearly zero after (including) k=3
# compute the theoretical ACF from an ARMA TS
plot(ARMAacf(ar = 0, ma=c(1.0, -0.5), lag.max=20, pacf=FALSE))
plot(ARMAacf(ar = 0, ma=c(1.0, -0.5), lag.max=20, pacf=TRUE))
arima(sp_prc, order=c(1,1,0))
arima(sp_prc, order=c(1,0,0))
arima(sp_prc, order=c(1,1,1))
abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwx
arima(sp_prc, order=c(1,1,0))
arima(sp_prc, order=c(1,0,0))
arima(sp_prc, order=c(1,1,1))
fit<-arima(sp_prc, order=c(2,1,0))
summary(fit)
####################
####  Forecast
####################
#install.packages('forecast')
library(forecast)
output<-forecast(fit,h=252)
plot(output)
names(output)
head(output$lower)
fit <- auto.arima(sp_prc, test='adf')
summary(fit)
fitted.values(fit)
residuals(fit)
output<-forecast(fit,h=252)
plot(output)
plot(naive(sp_prc[1:4000],h=200))
plot(naive(sp_prc[1:4000],h=200))
L <- length(ret_sp500_daily)-5000  # shrink the length to avoid row index error
x <- ret_sp500_daily[1:L]
fit2<-auto.arima(x)
fc <- forecast(fit2, h=200)  # the output would have error if the number of row > 1e4
names(fc)
accuracy(fc, ret_sp500_daily[(L+1):(L+200)])
L    <- length(sp_prc) - 5000
x    <- sp_prc[1:L]
fit.adf <- auto.arima(x,test='adf')
fc.adf  <- forecast(fit.adf, h=200)
accuracy(fc.adf, sp_prc[(L+1):(L+200)])
plot(fc.adf)
fit.kpss <- auto.arima(x)  # using the default kpss test
fc.kpss  <- forecast(fit.kpss, h=200)
accuracy(fc.kpss, sp_prc[(L+1):(L+200)])
plot(fc.kpss)
nn_model <- nnetar(ret_sp500_daily[2:L,1])
nn_model$p     # 33
nn_model$P     # 0
nn_model$size  #17 = round((33+0+1)/2) neuron size
fitted.nn<-fitted.nnetar(nn_model)
residuals.nnetar<-residuals.nnetar(nn_model)
sqrt(mean(residuals.nnetar**2,na.rm=T))  # 0.007213
accuracy(nn_model)
fc_nn <- forecast(nn_model, h=200)
accuracy(fc_nn, ret_sp500_daily[(L+1):(L+200), 1])
shiny::runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
runApp('git_project/mjc_Shiny_Project')
library(dplyr)
library(ggplot2)
library(data.table)
library(R.utils)
raw_data_full <- fread("./311_Service_Requests_from_2015.csv",select = c("Unique Key", "Complaint Type",'Descriptor','Borough', 'Latitude','Longitude'), nrows=500000)
#raw_data_full <- as.data.frame(raw_data_full)
test1 <- (raw_data_full$`Complaint Type`)
runApp('git_project/mjc_Shiny_Project')
'heating', 'noise','construction','plumbing', 'paint', 'unsanitary', 'car_related','street_lights_signals','tree','electric','vermin','street_sidewalk'
'heating', 'noise','construction','plumbing', 'paint', 'unsanitary', 'car_related','street_lights_signals','tree','electric','vermin','street_sidewalk'
runApp('git_project/mjc_Shiny_Project')
